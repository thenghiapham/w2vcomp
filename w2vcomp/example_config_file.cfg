# Model parameters, part 1

# main project dir
ProjectDir           = /mnt/cimec-storage-sata/users/thenghia.pham/data/project/mikcom

# construction file, consisting list of syntactic constructions that should be taken into
# account, the remaining constructions will be merged together.
# pass an empty file if you run the C-Phrase model, since C-Phrase treats every construct
# the same
# the file's path is assume to be ProjectDir/ConstructionFileName
 
ConstructionFileName = constructions/no-constructions.txt

# Training Directory, which consists of parsed data files
# Note 1: the number of training files in this repository will
# be the number of threads the program will use
# 8 can be a good number of threads if your computer can handle it
# Note 2: the format of the training file is simple,
# one line should consist of 1 parse tree (output from the binarized stanford parser)
STrainDirName        = corpus/split/bwu_20

# Min frequency of words in vocabulary
MinFrequency         = 100

# Output Filename template
# The real output filename will be obtained by replacing istep, type, ... with the real values
SOutputFileTemplate  = s_neg_istep_type_add1_bwu_size_window_function_wo_lex.bin

# Vocab file name, if vocab file does not exist, the program will create one from the corpus
# similar to word2vec's vocab file
VocabFileName        = bwu_lower1.voc

# Not used, but should keep untouched
WordVectorFileName   = neg_skip_bwu_ws_size.bin



# Evaluation DATA PATHS
# Men file, while training, the program will evaluate the quality of the word embeddings by computing
# the correlation on this dataset    
MenFile              = /mnt/cimec-storage-sata/users/thenghia.pham/data/project/mikcom/men/MEN_dataset_lemma.txt

# Not used
SickFile             = /mnt/cimec-storage-sata/users/thenghia.pham/data/project/mikcom/sick/parsed/SICK_train_trial.txt
DatasetDir           = /mnt/cimec-storage-sata/users/thenghia.pham/data/project/mikcom/lapata



# Model parameters, part 2
HierarchialSoftmax   = false
NegativeSampling     = 10
SubSampling          = 1e-5
# Maximum phrase level for training?
PhraseLevel          = 6
# Use all the level up to PhraseLevel?
AllLevel             = true
# Use the single word level?
Lexical              = false